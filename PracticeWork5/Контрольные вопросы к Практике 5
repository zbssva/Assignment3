1. В чём отличие стека и очереди?

Стек (LIFO — Last In First Out): добавление и удаление элементов происходит с одного конца (вершины). Последний добавленный элемент извлекается первым.

Очередь (FIFO — First In First Out): добавление элементов происходит в конец, а удаление — из начала. Первый добавленный элемент извлекается первым.
Пример:

Стек: если положить 1, 2, 3 → извлекаем 3, 2, 1

Очередь: если положить 1, 2, 3 → извлекаем 1, 2, 3

2. Какие проблемы возникают при параллельном доступе к данным?

Конфликты записи: несколько потоков одновременно пытаются изменить один и тот же элемент → данные могут быть повреждены.

Гонка данных (data race): результат зависит от порядка выполнения потоков.

Неправильный порядок операций: push/pop или enqueue/dequeue могут работать некорректно без синхронизации.

3. Как атомарные операции помогают избежать конфликтов?

Атомарные операции выполняются как неделимая единица: пока поток не завершит операцию, другой поток не сможет вмешаться.

Примеры в CUDA: atomicAdd, atomicSub.

В стеке: атомарный atomicAdd(&top, 1) гарантирует, что каждый поток получит уникальную позицию для push.

В очереди: атомарный atomicAdd(&tail, 1) гарантирует корректное добавление в конец очереди без конфликтов.

4. Какие типы памяти CUDA используются для хранения данных?

Глобальная память (global memory): доступна всем блокам и потокам, но медленный доступ.

Разделяемая память (shared memory): общая для потоков одного блока, очень быстрая, но ограниченного размера.

Локальная память (local memory): приватная для каждого потока, хранится в регистре или в памяти GPU, медленнее, чем shared memory.

5. Как синхронизация потоков влияет на производительность?

Синхронизация (__syncthreads()) обеспечивает корректный порядок выполнения операций между потоками одного блока.

Без синхронизации может возникнуть гонка данных.

Минус: слишком частая синхронизация замедляет работу, т.к. потоки ждут друг друга. Нужно синхронизировать только там, где это необходимо.

6. Почему разделяемая память важна для оптимизации работы параллельных структур данных?

Разделяемая память быстрее глобальной, так как находится в блоке потоков на GPU.

Позволяет потокам одного блока быстро обмениваться данными.

Снижает количество обращений к медленной глобальной памяти → ускоряет push/pop и enqueue/dequeue.

Часто используется для промежуточных буферов и временных результатов перед записью в глобальную память.
